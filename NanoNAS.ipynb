{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObEbzwAau0Mp0d5gPPx3WS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjanascorner/neural-architecture-search/blob/master/NanoNAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0CmHGqVYq-Ga"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import Process,Queue\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import datetime\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x stm32tflm\n"
      ],
      "metadata": {
        "id": "aqGiQOO3fMzh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoNAS :\n",
        "    def __init__(self, max_ram, max_flash, max_macc, path_to_training_set, val_split, cache=False, input_shape=(50,50,3), save_path='./') :\n",
        "        self.path_to_training_set = path_to_training_set\n",
        "        self.num_classes = len(next(os.walk(path_to_training_set))[1])\n",
        "        self.val_split = val_split\n",
        "        self.input_shape = input_shape\n",
        "        self.max_ram = max_ram\n",
        "        self.max_flash = max_flash\n",
        "        self.max_macc = max_macc\n",
        "        self.cache = cache\n",
        "        self.save_path = Path(save_path)\n",
        "        os.makedirs(self.save_path,exist_ok=True)\n",
        "        self.path_to_resulting_model = self.save_path / 'resulting_architecture.h5'\n",
        "        self.path_to_quantized_resulting_model = self.save_path / 'resulting_architecture.tflite'\n",
        "\n",
        "    # k number of kernels of the first convolutional layer\n",
        "    # c number of cells added upon the first convolutional layer\n",
        "    # pre-processing pipeline not included in MACC computation\n",
        "    def Model(self, k, c) :\n",
        "        kernel_size = (3,3)\n",
        "        pool_size = (2,2)\n",
        "        pool_strides = (2,2)\n",
        "\n",
        "        number_of_cells_limited = False\n",
        "        macc = 0\n",
        "\n",
        "        inputs = tf.keras.Input(shape=self.input_shape)\n",
        "\n",
        "        #convolutional base\n",
        "        n = k\n",
        "        multiplier = 2\n",
        "\n",
        "        #first convolutional layer\n",
        "        c_in = self.input_shape[2]\n",
        "        x = tf.keras.layers.Conv2D(n, kernel_size, padding='same')(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation('relu')(x)\n",
        "        macc = macc + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        #adding cells\n",
        "        for i in range(1, c + 1) :\n",
        "            if x.shape[1] <= 1 or x.shape[2] <= 1 :\n",
        "                number_of_cells_limited = True\n",
        "                break;\n",
        "            n = int(np.ceil(n * multiplier))\n",
        "            multiplier = multiplier - 2**-i\n",
        "            x = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_strides, padding='valid')(x)\n",
        "            x = tf.keras.layers.Conv2D(n, kernel_size, padding='same')(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Activation('relu')(x)\n",
        "            c_in = x.shape[3]\n",
        "            macc = macc + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        #classifier\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        x = tf.keras.layers.Dropout(0.5)(x)\n",
        "        outputs = tf.keras.layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "        macc = macc + (x.shape[1] * outputs.shape[1])\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        return model, macc, number_of_cells_limited\n",
        "\n",
        "    def load_training_set(self, batch_size=1):\n",
        "        if 3 == self.input_shape[2] :\n",
        "            color_mode = 'rgb'\n",
        "        elif 1 == self.input_shape[2] :\n",
        "            color_mode = 'grayscale'\n",
        "\n",
        "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        if self.cache :\n",
        "            train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        else :\n",
        "            train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            validation_ds = validation_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        return train_ds, validation_ds\n",
        "\n",
        "    def compile_model(self, model, learning_rate):\n",
        "         opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "         model.compile(optimizer=opt,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    def quantize_model(self, model, train_ds, path_to_tflite_model) :\n",
        "        def representative_dataset():\n",
        "            for data in train_ds.rebatch(1).take(150) :\n",
        "                yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.representative_dataset = representative_dataset\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8  # or tf.int8\n",
        "        converter.inference_output_type = tf.uint8  # or tf.int8\n",
        "        tflite_quant_model = converter.convert()\n",
        "\n",
        "        with open(path_to_tflite_model, 'wb') as f:\n",
        "            f.write(tflite_quant_model)\n",
        "\n",
        "    def evaluate_flash_and_peak_ram_occupancy(self, model, train_ds) :\n",
        "        #it must be done after one epoch of training, at least\n",
        "        path_to_tflite_model = self.save_path / 'temp.tflite'\n",
        "        #quantize model to evaluate its peak RAM occupancy and its Flash occupancy\n",
        "        self.quantize_model(model, train_ds, path_to_tflite_model)\n",
        "\n",
        "        #evaluate its peak RAM occupancy and its Flash occupancy using STMicroelectronics' script named \"stm32tflm\"\n",
        "        #found inside the linux package of X-CUBE-AI at the following path:\n",
        "        #\"path/to/en.x-cube-ai-linux_v8.0.1/stm32ai-linux-8.0.1/linux/stm32tflm\".\n",
        "        #The package can be downloaded at https://www.st.com/en/embedded-software/x-cube-ai.html#get-software.\n",
        "        proc = subprocess.Popen([\"./stm32tflm\", path_to_tflite_model], stdout=subprocess.PIPE)\n",
        "        try:\n",
        "            outs, errs = proc.communicate(timeout=15)\n",
        "            flash, ram = re.findall(r'\\d+', str(outs))\n",
        "            os.remove(path_to_tflite_model)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            outs, errs = proc.communicate()\n",
        "            os.remove(path_to_tflite_model)\n",
        "            print(\"stm32tflm error\")\n",
        "            exit()\n",
        "\n",
        "        return int(flash), int(ram)\n",
        "\n",
        "    def evaluate_model_process(self, q, k, c) :\n",
        "        epochs = 3\n",
        "        search_learning_rate = 0.001\n",
        "        search_batch_size = 16\n",
        "\n",
        "        train_ds, validation_ds = self.load_training_set(search_batch_size)\n",
        "\n",
        "        model, macc, number_of_cells_limited = self.Model(k, c)\n",
        "        self.compile_model(model, search_learning_rate)\n",
        "        hist = model.fit(train_ds, epochs=epochs, validation_data=validation_ds, validation_freq=1)\n",
        "        flash, ram = self.evaluate_flash_and_peak_ram_occupancy(model, train_ds)\n",
        "        feasible = macc <= self.max_macc and flash <= self.max_flash and ram <= self.max_ram and not number_of_cells_limited\n",
        "        if self.save_search_history and feasible :\n",
        "            model_name = 'k' + str(k) + '_c' + str(c)\n",
        "            new_dir = self.save_path / 'search_history' / f\"search_learning_rate_{search_learning_rate}\" / f\"search_batch_size{search_batch_size}\" / model_name\n",
        "            os.makedirs(new_dir)\n",
        "            np.save(new_dir / f\"{model_name}_hist.npy\", hist.history)\n",
        "        q.put({'k': k,\n",
        "               'c': c if not number_of_cells_limited else f\"{c} (Not feasible)\",\n",
        "               'RAM': ram if ram <= self.max_ram else f\"{ram} (Outside the upper bound of {ram - self.max_ram} Byte)\",\n",
        "               'Flash': flash if flash <= self.max_flash else f\"{flash} (Outside the upper bound of {flash - self.max_flash} Byte)\",\n",
        "               'MACC': macc if macc <= self.max_macc else f\"{macc} (Outside the upper bound of {macc - self.max_macc} MAC)\",\n",
        "               'max_val_acc': np.around(np.amax(hist.history['val_accuracy']), decimals=3)\n",
        "               if feasible else -3})\n",
        "\n",
        "    def search(self, save_search_history=False) :\n",
        "        self.save_search_history = save_search_history\n",
        "\n",
        "        start = datetime.datetime.now()\n",
        "\n",
        "        best_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
        "        new_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
        "\n",
        "        k = 1\n",
        "        while(new_architecture['max_val_acc'] > best_architecture['max_val_acc']) :\n",
        "            best_architecture = new_architecture\n",
        "            c = -1\n",
        "            previous_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
        "            current_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
        "            while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "                previous_architecture = current_architecture\n",
        "                c = c + 1\n",
        "                q = Queue()\n",
        "                p = Process(target=self.evaluate_model_process, args=(q, k, c,))\n",
        "                p.start()\n",
        "                p.join()\n",
        "                if q.empty() :\n",
        "                    #the machine was not able to train the architecture for one epoch\n",
        "                    current_architecture = {'k': k, 'c': c, 'max_val_acc': -1}\n",
        "                else :\n",
        "                    current_architecture = q.get()\n",
        "                print(f\"\\n\\n{current_architecture}\\n\\n\")\n",
        "            new_architecture = previous_architecture\n",
        "            k = k + 1\n",
        "\n",
        "        end = datetime.datetime.now()\n",
        "\n",
        "        if 0 < best_architecture['max_val_acc'] :\n",
        "            print(f\"Resulting architecture: {best_architecture}\\n\")\n",
        "            print(f\"Elapsed time (search): {end-start}\\n\")\n",
        "\n",
        "            self.resulting_architecture = best_architecture\n",
        "        else :\n",
        "            print(\"No feasible solution found.\\n\")\n",
        "            exit(0)\n",
        "\n",
        "    def train_process(self, training_epochs, training_learning_rate, training_batch_size) :\n",
        "        train_ds, validation_ds = self.load_training_set(training_batch_size)\n",
        "        model = self.Model(self.resulting_architecture['k'], self.resulting_architecture['c'])[0]\n",
        "        self.compile_model(model, training_learning_rate)\n",
        "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath= str(self.path_to_resulting_model),\n",
        "            save_weights_only=False, monitor='val_accuracy',\n",
        "            mode='auto', save_best_only=True, verbose=1)\n",
        "\n",
        "        hist = model.fit(train_ds, epochs=training_epochs, validation_data=validation_ds, validation_freq=1, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "        print('\\nmax val acc: ' + str(round(np.amax(hist.history['val_accuracy']), 3)))\n",
        "\n",
        "        print(f\"\\nKeras model saved in: {self.path_to_resulting_model}\\n\")\n",
        "\n",
        "    def train(self, training_epochs, training_learning_rate, training_batch_size) :\n",
        "        start = datetime.datetime.now()\n",
        "        p = Process(target=self.train_process, args=((training_epochs, training_learning_rate, training_batch_size,)))\n",
        "        p.start()\n",
        "        p.join()\n",
        "        end = datetime.datetime.now()\n",
        "        print(f\"Elapsed time (training): {end-start}\\n\")\n",
        "\n",
        "    def apply_uint8_post_training_quantization_process(self) :\n",
        "        train_ds, validation_ds = self.load_training_set()\n",
        "\n",
        "        model = tf.keras.models.load_model(self.path_to_resulting_model)\n",
        "\n",
        "        self.quantize_model(model, train_ds, self.path_to_quantized_resulting_model)\n",
        "\n",
        "        print(f\"\\nTflite model saved in: {self.path_to_quantized_resulting_model.resolve()}\\n\")\n",
        "\n",
        "    def apply_uint8_post_training_quantization(self) :\n",
        "        p = Process(target=self.apply_uint8_post_training_quantization_process)\n",
        "        p.start()\n",
        "        p.join()\n",
        "\n",
        "    def load_test_set(self, path_to_test_set, batch_size=1):\n",
        "        if 3 == self.input_shape[2] :\n",
        "            color_mode = 'rgb'\n",
        "        elif 1 == self.input_shape[2] :\n",
        "            color_mode = 'grayscale'\n",
        "\n",
        "        test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= path_to_test_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=batch_size,\n",
        "            image_size=self.input_shape[0:2]\n",
        "        )\n",
        "\n",
        "        if self.cache :\n",
        "            test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        else :\n",
        "            test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        return test_ds\n",
        "\n",
        "    def test_keras_model_process(self, path_to_test_set) :\n",
        "        test_ds = self.load_test_set(path_to_test_set)\n",
        "        model = tf.keras.models.load_model(self.path_to_resulting_model)\n",
        "\n",
        "        #evaluate keras model\n",
        "        print(f\"\\nKeras model test accuracy: {model.evaluate(test_ds)[1]}\\n\")\n",
        "\n",
        "    def test_keras_model(self, path_to_test_set) :\n",
        "        p = Process(target=self.test_keras_model_process, args=(path_to_test_set,))\n",
        "        p.start()\n",
        "        p.join()\n",
        "\n",
        "    def test_tflite_model_process(self, path_to_test_set) :\n",
        "        test_ds = self.load_test_set(path_to_test_set)\n",
        "        interpreter = tf.lite.Interpreter(f\"{self.path_to_quantized_resulting_model.resolve()}\")\n",
        "        interpreter.allocate_tensors()\n",
        "\n",
        "        output = interpreter.get_output_details()[0]  # Model has single output.\n",
        "        input = interpreter.get_input_details()[0]  # Model has single input.\n",
        "\n",
        "        correct = 0\n",
        "        wrong = 0\n",
        "\n",
        "        for image, label in test_ds.rebatch(1) :\n",
        "            # Check if the input type is quantized, then rescale input data to uint8\n",
        "            if input['dtype'] == tf.uint8:\n",
        "                input_scale, input_zero_point = input[\"quantization\"]\n",
        "                image = image / input_scale + input_zero_point\n",
        "            input_data = tf.dtypes.cast(image, tf.uint8)\n",
        "            interpreter.set_tensor(input['index'], input_data)\n",
        "            interpreter.invoke()\n",
        "            if label.numpy().argmax() == interpreter.get_tensor(output['index']).argmax() :\n",
        "                correct = correct + 1\n",
        "            else :\n",
        "                wrong = wrong + 1\n",
        "        print(f\"\\nTflite model test accuracy: {correct/(correct+wrong)}\")\n",
        "\n",
        "        print(f\"\\nTflite model in: {self.path_to_quantized_resulting_model.resolve()}\\n\")\n",
        "\n",
        "    def test_tflite_model(self, path_to_test_set) :\n",
        "        p = Process(target=self.test_tflite_model_process, args=(path_to_test_set,))\n",
        "        p.start()\n",
        "        p.join()"
      ],
      "metadata": {
        "id": "m5JxlVMksyXx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "0mjk5TOVZUKq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images\n",
        "!unzip melanoma-skin-cancer-dataset-of-10000-images.zip\n"
      ],
      "metadata": {
        "id": "M3xT3Idcfh7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "input_shape = (50,50,3)\n",
        "\n",
        "#Each dataset must comply with the following structure\n",
        "#main_directory/\n",
        "#...class_a/\n",
        "#......a_image_1.jpg\n",
        "#......a_image_2.jpg\n",
        "#...class_b/\n",
        "#......b_image_1.jpg\n",
        "#......b_image_2.jpg\n",
        "path_to_training_set = r\"/content/melanoma_cancer_dataset/train\"\n",
        "val_split = 0.3\n",
        "path_to_test_set=r\"/content/melanoma_cancer_dataset/test\"\n",
        "\n",
        "#whether or not to cache datasets in memory\n",
        "#if the dataset cannot fit in the main memory, the application will crash\n",
        "cache = True\n",
        "\n",
        "#target: STM32L010RBT6\n",
        "#75 CoreMark, 20 kiB RAM, 128 kiB Flash\n",
        "ram_upper_bound = 20480\n",
        "flash_upper_bound = 131072\n",
        "MACC_upper_bound = 750000 #CoreMark * 1e4\n",
        "\n",
        "nanoNAS = NanoNAS(ram_upper_bound, flash_upper_bound, MACC_upper_bound, path_to_training_set, val_split, cache, input_shape, save_path='./results')\n",
        "\n",
        "#search\n",
        "nanoNAS.search(save_search_history=False)\n",
        "\n",
        "#train resulting architecture\n",
        "nanoNAS.train(training_epochs=100, training_learning_rate=0.01, training_batch_size=128)\n",
        "\n",
        "#apply uint8 post trainig quantization\n",
        "nanoNAS.apply_uint8_post_training_quantization()\n",
        "\n",
        "#evaluate post training quantization\n",
        "nanoNAS.test_keras_model(path_to_test_set)\n",
        "nanoNAS.test_tflite_model(path_to_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjMgJoR9Rzjn",
        "outputId": "305d9280-1aa8-4b0d-d77b-4c8a7413db40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.4796 - loss: 0.6897 - val_accuracy: 0.4825 - val_loss: 0.6939\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5756 - loss: 0.6505 - val_accuracy: 0.6012 - val_loss: 0.5982\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.6185 - loss: 0.6332 - val_accuracy: 0.7605 - val_loss: 0.5796\n",
            "Saved artifact at '/tmp/tmpre2lr5vt'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277337040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 1, 'c': 0, 'RAM': 17408, 'Flash': 3120, 'MACC': 67502, 'max_val_acc': np.float64(0.76)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.5630 - loss: 0.7110 - val_accuracy: 0.6286 - val_loss: 0.6197\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.7152 - loss: 0.5823 - val_accuracy: 0.7397 - val_loss: 0.5376\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.7213 - loss: 0.5750 - val_accuracy: 0.7241 - val_loss: 0.5427\n",
            "Saved artifact at '/tmp/tmphbetj1ho'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277091280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277091856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277091472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277092816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277091664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277092624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277094928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277096656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277097232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 1, 'c': 1, 'RAM': 17408, 'Flash': 4216, 'MACC': 90004, 'max_val_acc': np.float64(0.74)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.5670 - loss: 0.7077 - val_accuracy: 0.7709 - val_loss: 0.5831\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.7128 - loss: 0.5904 - val_accuracy: 0.7754 - val_loss: 0.5350\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7071 - loss: 0.5796 - val_accuracy: 0.7619 - val_loss: 0.5300\n",
            "Saved artifact at '/tmp/tmpsnlqow8n'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277337424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 2, 'c': 0, 'RAM': 17408, 'Flash': 3184, 'MACC': 135004, 'max_val_acc': np.float64(0.775)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.5715 - loss: 0.6667 - val_accuracy: 0.6845 - val_loss: 0.5916\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.7380 - loss: 0.5475 - val_accuracy: 0.7595 - val_loss: 0.5010\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.7477 - loss: 0.5333 - val_accuracy: 0.7973 - val_loss: 0.4704\n",
            "Saved artifact at '/tmp/tmp6cco4rja'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277321040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277324688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277326416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277326992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 2, 'c': 1, 'RAM': 17408, 'Flash': 4392, 'MACC': 225008, 'max_val_acc': np.float64(0.797)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 53ms/step - accuracy: 0.6425 - loss: 0.6715 - val_accuracy: 0.8143 - val_loss: 0.4556\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 41ms/step - accuracy: 0.7936 - loss: 0.4804 - val_accuracy: 0.8334 - val_loss: 0.3926\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.7999 - loss: 0.4577 - val_accuracy: 0.8501 - val_loss: 0.3751\n",
            "Saved artifact at '/tmp/tmp8zi0wm88'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277238736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277239312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277241424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277241040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277238928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277240272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277240464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277241232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277239120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277240080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277240848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277242384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277243536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277244112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277246032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277246224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277245456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277243920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277244496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277245648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 2, 'c': 2, 'RAM': 17920, 'Flash': 5760, 'MACC': 271668, 'max_val_acc': np.float64(0.85)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 69ms/step - accuracy: 0.6563 - loss: 0.6090 - val_accuracy: 0.8403 - val_loss: 0.3650\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.8143 - loss: 0.4260 - val_accuracy: 0.8438 - val_loss: 0.3559\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.8169 - loss: 0.3994 - val_accuracy: 0.8334 - val_loss: 0.3653\n",
            "Saved artifact at '/tmp/tmp7zeruxkc'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277337040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277341840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277342416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277343760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277342224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277345872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277345680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277345104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277346256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277346064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 2, 'c': 3, 'RAM': 18432, 'Flash': 7408, 'MACC': 292408, 'max_val_acc': np.float64(0.844)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.5989 - loss: 0.6100 - val_accuracy: 0.7397 - val_loss: 0.5096\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.6234 - loss: 0.5662 - val_accuracy: 0.7136 - val_loss: 0.5232\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.6697 - loss: 0.5609 - val_accuracy: 0.7765 - val_loss: 0.4730\n",
            "Saved artifact at '/tmp/tmp0mhqawzd'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277337040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 3, 'c': 0, 'RAM': 17408, 'Flash': 3248, 'MACC': 202506, 'max_val_acc': np.float64(0.776)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.6187 - loss: 0.6651 - val_accuracy: 0.6314 - val_loss: 0.7412\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.7375 - loss: 0.5404 - val_accuracy: 0.6897 - val_loss: 0.5980\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.7438 - loss: 0.5328 - val_accuracy: 0.7917 - val_loss: 0.4819\n",
            "Saved artifact at '/tmp/tmpi3kgo1ls'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277320656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277320848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277324304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277326032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277326608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 3, 'c': 1, 'RAM': 17920, 'Flash': 4600, 'MACC': 405012, 'max_val_acc': np.float64(0.792)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.6433 - loss: 0.6301 - val_accuracy: 0.8199 - val_loss: 0.4090\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.8031 - loss: 0.4468 - val_accuracy: 0.8386 - val_loss: 0.3765\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 43ms/step - accuracy: 0.8164 - loss: 0.4226 - val_accuracy: 0.8587 - val_loss: 0.3339\n",
            "Saved artifact at '/tmp/tmplmpu2gw9'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277320656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277320848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277324304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277325456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277326032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277327952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277328144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277327376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277325840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277326416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277327568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 3, 'c': 2, 'RAM': 18432, 'Flash': 6328, 'MACC': 509994, 'max_val_acc': np.float64(0.859)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 63ms/step - accuracy: 0.7329 - loss: 0.5471 - val_accuracy: 0.8594 - val_loss: 0.3314\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - accuracy: 0.8445 - loss: 0.3959 - val_accuracy: 0.8684 - val_loss: 0.3637\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.8465 - loss: 0.3656 - val_accuracy: 0.8823 - val_loss: 0.2870\n",
            "Saved artifact at '/tmp/tmpz8qhukdx'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277418960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277419536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277421648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277421264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277419152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277420496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277420688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277421456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277419344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277420304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277421072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277422608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277423760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277424336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277426256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277426448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277425680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277424144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277426832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277426640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277426064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277427792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277427600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277427024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277428176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277427984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 3, 'c': 3, 'RAM': 18944, 'Flash': 8624, 'MACC': 556656, 'max_val_acc': np.float64(0.882)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 61ms/step - accuracy: 0.7857 - loss: 0.4572 - val_accuracy: 0.8716 - val_loss: 0.3046\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.8459 - loss: 0.3616 - val_accuracy: 0.8632 - val_loss: 0.3319\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.8699 - loss: 0.3202 - val_accuracy: 0.8775 - val_loss: 0.2902\n",
            "Saved artifact at '/tmp/tmpciufo66y'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277091280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277091856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277091472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277092816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277091664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277092624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277093392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277094928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277096080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277096656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277098576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277098768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277098000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277096464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277099152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277098960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277098384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277100112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277099920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277099344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277099728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277098192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277101456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277101648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277100496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277100304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277102032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277102416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 3, 'c': 4, 'RAM': 19456, 'Flash': 11520, 'MACC': 572536, 'max_val_acc': np.float64(0.877)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5719 - loss: 0.6369 - val_accuracy: 0.7025 - val_loss: 0.5338\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.6789 - loss: 0.5367 - val_accuracy: 0.7563 - val_loss: 0.4886\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.6869 - loss: 0.5392 - val_accuracy: 0.7737 - val_loss: 0.4695\n",
            "Saved artifact at '/tmp/tmpexxoirvb'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277320656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277321232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277320848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277322192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277323728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 4, 'c': 0, 'RAM': 19968, 'Flash': 3296, 'MACC': 270008, 'max_val_acc': np.float64(0.774)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.6680 - loss: 0.6024 - val_accuracy: 0.8001 - val_loss: 0.4449\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.7762 - loss: 0.4922 - val_accuracy: 0.8094 - val_loss: 0.4258\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.7772 - loss: 0.4826 - val_accuracy: 0.8115 - val_loss: 0.4208\n",
            "Saved artifact at '/tmp/tmpoxtdj_zj'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688276534224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276534800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276536912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276536528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276534416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276535760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276535952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276536720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276534608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276535568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276536336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276537872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276539600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688276540176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 4, 'c': 1, 'RAM': 20480, 'Flash': 4832, 'MACC': 630016, 'max_val_acc': np.float64(0.812)}\n",
            "\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7374 - loss: 0.5183 - val_accuracy: 0.7171 - val_loss: 0.4886\n",
            "Epoch 2/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.8125 - loss: 0.4290 - val_accuracy: 0.8042 - val_loss: 0.4280\n",
            "Epoch 3/3\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 43ms/step - accuracy: 0.8189 - loss: 0.3944 - val_accuracy: 0.8178 - val_loss: 0.4294\n",
            "Saved artifact at '/tmp/tmpghmnq0x3'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277337040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277337424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277338384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277339152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277340688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277341840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277342416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277344528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277343760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277342224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277342800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277343952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'k': 4, 'c': 2, 'RAM': '20992 (Outside the upper bound of 512 Byte)', 'Flash': 7024, 'MACC': '816648 (Outside the upper bound of 66648 MAC)', 'max_val_acc': -3}\n",
            "\n",
            "\n",
            "Resulting architecture: {'k': 3, 'c': 3, 'RAM': 18944, 'Flash': 8624, 'MACC': 556656, 'max_val_acc': np.float64(0.882)}\n",
            "\n",
            "Elapsed time (search): 0:15:30.336155\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n",
            "Epoch 1/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.7784 - loss: 0.4919\n",
            "Epoch 1: val_accuracy improved from -inf to 0.52343, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 406ms/step - accuracy: 0.7790 - loss: 0.4910 - val_accuracy: 0.5234 - val_loss: 5.3432\n",
            "Epoch 2/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8638 - loss: 0.3468\n",
            "Epoch 2: val_accuracy improved from 0.52343 to 0.53905, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 258ms/step - accuracy: 0.8638 - loss: 0.3466 - val_accuracy: 0.5390 - val_loss: 3.3516\n",
            "Epoch 3/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8720 - loss: 0.3211\n",
            "Epoch 3: val_accuracy improved from 0.53905 to 0.61333, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 268ms/step - accuracy: 0.8720 - loss: 0.3209 - val_accuracy: 0.6133 - val_loss: 2.0472\n",
            "Epoch 4/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8667 - loss: 0.3153\n",
            "Epoch 4: val_accuracy improved from 0.61333 to 0.77091, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 232ms/step - accuracy: 0.8667 - loss: 0.3152 - val_accuracy: 0.7709 - val_loss: 0.5505\n",
            "Epoch 5/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8759 - loss: 0.2987\n",
            "Epoch 5: val_accuracy improved from 0.77091 to 0.83513, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 248ms/step - accuracy: 0.8760 - loss: 0.2987 - val_accuracy: 0.8351 - val_loss: 0.4102\n",
            "Epoch 6/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8731 - loss: 0.2907\n",
            "Epoch 6: val_accuracy improved from 0.83513 to 0.84450, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 249ms/step - accuracy: 0.8731 - loss: 0.2906 - val_accuracy: 0.8445 - val_loss: 0.3858\n",
            "Epoch 7/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8826 - loss: 0.2915\n",
            "Epoch 7: val_accuracy did not improve from 0.84450\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.8826 - loss: 0.2915 - val_accuracy: 0.7966 - val_loss: 0.5053\n",
            "Epoch 8/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8910 - loss: 0.2836\n",
            "Epoch 8: val_accuracy improved from 0.84450 to 0.87608, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - accuracy: 0.8909 - loss: 0.2836 - val_accuracy: 0.8761 - val_loss: 0.3229\n",
            "Epoch 9/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8902 - loss: 0.2785\n",
            "Epoch 9: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - accuracy: 0.8902 - loss: 0.2784 - val_accuracy: 0.6161 - val_loss: 1.7376\n",
            "Epoch 10/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8876 - loss: 0.2717\n",
            "Epoch 10: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.8876 - loss: 0.2717 - val_accuracy: 0.8442 - val_loss: 0.3799\n",
            "Epoch 11/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8907 - loss: 0.2724\n",
            "Epoch 11: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 238ms/step - accuracy: 0.8906 - loss: 0.2725 - val_accuracy: 0.7338 - val_loss: 0.6855\n",
            "Epoch 12/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8881 - loss: 0.2724\n",
            "Epoch 12: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.8881 - loss: 0.2724 - val_accuracy: 0.8428 - val_loss: 0.4338\n",
            "Epoch 13/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8917 - loss: 0.2793\n",
            "Epoch 13: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 236ms/step - accuracy: 0.8917 - loss: 0.2792 - val_accuracy: 0.7171 - val_loss: 0.9074\n",
            "Epoch 14/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8964 - loss: 0.2720\n",
            "Epoch 14: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 248ms/step - accuracy: 0.8964 - loss: 0.2720 - val_accuracy: 0.7754 - val_loss: 0.6761\n",
            "Epoch 15/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8974 - loss: 0.2573\n",
            "Epoch 15: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 253ms/step - accuracy: 0.8973 - loss: 0.2573 - val_accuracy: 0.5574 - val_loss: 3.3043\n",
            "Epoch 16/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8936 - loss: 0.2642\n",
            "Epoch 16: val_accuracy did not improve from 0.87608\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 250ms/step - accuracy: 0.8936 - loss: 0.2642 - val_accuracy: 0.7966 - val_loss: 0.7140\n",
            "Epoch 17/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8995 - loss: 0.2547\n",
            "Epoch 17: val_accuracy improved from 0.87608 to 0.88650, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 255ms/step - accuracy: 0.8995 - loss: 0.2546 - val_accuracy: 0.8865 - val_loss: 0.2864\n",
            "Epoch 18/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.9071 - loss: 0.2507\n",
            "Epoch 18: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 249ms/step - accuracy: 0.9071 - loss: 0.2507 - val_accuracy: 0.7272 - val_loss: 1.0233\n",
            "Epoch 19/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8982 - loss: 0.2582\n",
            "Epoch 19: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - accuracy: 0.8982 - loss: 0.2580 - val_accuracy: 0.7813 - val_loss: 0.7824\n",
            "Epoch 20/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9049 - loss: 0.2483\n",
            "Epoch 20: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 248ms/step - accuracy: 0.9048 - loss: 0.2482 - val_accuracy: 0.8528 - val_loss: 0.3879\n",
            "Epoch 21/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9000 - loss: 0.2512\n",
            "Epoch 21: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.9000 - loss: 0.2511 - val_accuracy: 0.6744 - val_loss: 1.1920\n",
            "Epoch 22/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9019 - loss: 0.2465\n",
            "Epoch 22: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 250ms/step - accuracy: 0.9020 - loss: 0.2464 - val_accuracy: 0.8219 - val_loss: 0.5794\n",
            "Epoch 23/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9022 - loss: 0.2494\n",
            "Epoch 23: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 249ms/step - accuracy: 0.9022 - loss: 0.2492 - val_accuracy: 0.7768 - val_loss: 0.6285\n",
            "Epoch 24/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9078 - loss: 0.2390\n",
            "Epoch 24: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9077 - loss: 0.2390 - val_accuracy: 0.7372 - val_loss: 1.0392\n",
            "Epoch 25/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9035 - loss: 0.2406\n",
            "Epoch 25: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 276ms/step - accuracy: 0.9036 - loss: 0.2404 - val_accuracy: 0.8351 - val_loss: 0.5137\n",
            "Epoch 26/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.9112 - loss: 0.2343\n",
            "Epoch 26: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 241ms/step - accuracy: 0.9111 - loss: 0.2343 - val_accuracy: 0.6765 - val_loss: 1.3175\n",
            "Epoch 27/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9062 - loss: 0.2418\n",
            "Epoch 27: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 251ms/step - accuracy: 0.9062 - loss: 0.2417 - val_accuracy: 0.8292 - val_loss: 0.4757\n",
            "Epoch 28/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9012 - loss: 0.2525\n",
            "Epoch 28: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 251ms/step - accuracy: 0.9012 - loss: 0.2523 - val_accuracy: 0.8160 - val_loss: 0.5685\n",
            "Epoch 29/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9054 - loss: 0.2337\n",
            "Epoch 29: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 248ms/step - accuracy: 0.9054 - loss: 0.2336 - val_accuracy: 0.8254 - val_loss: 0.5307\n",
            "Epoch 30/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.9092 - loss: 0.2334\n",
            "Epoch 30: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 244ms/step - accuracy: 0.9093 - loss: 0.2334 - val_accuracy: 0.6914 - val_loss: 1.1222\n",
            "Epoch 31/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9070 - loss: 0.2275\n",
            "Epoch 31: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 258ms/step - accuracy: 0.9070 - loss: 0.2275 - val_accuracy: 0.8268 - val_loss: 0.4936\n",
            "Epoch 32/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.9126 - loss: 0.2239\n",
            "Epoch 32: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 234ms/step - accuracy: 0.9125 - loss: 0.2239 - val_accuracy: 0.8629 - val_loss: 0.3742\n",
            "Epoch 33/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9067 - loss: 0.2322\n",
            "Epoch 33: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 251ms/step - accuracy: 0.9067 - loss: 0.2321 - val_accuracy: 0.8244 - val_loss: 0.5303\n",
            "Epoch 34/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9093 - loss: 0.2309\n",
            "Epoch 34: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9093 - loss: 0.2308 - val_accuracy: 0.7758 - val_loss: 0.8870\n",
            "Epoch 35/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9110 - loss: 0.2202\n",
            "Epoch 35: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.9110 - loss: 0.2202 - val_accuracy: 0.7397 - val_loss: 0.9502\n",
            "Epoch 36/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9055 - loss: 0.2404\n",
            "Epoch 36: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 259ms/step - accuracy: 0.9055 - loss: 0.2404 - val_accuracy: 0.7893 - val_loss: 0.7498\n",
            "Epoch 37/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9112 - loss: 0.2251\n",
            "Epoch 37: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 257ms/step - accuracy: 0.9112 - loss: 0.2250 - val_accuracy: 0.7365 - val_loss: 0.8933\n",
            "Epoch 38/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9105 - loss: 0.2295\n",
            "Epoch 38: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - accuracy: 0.9105 - loss: 0.2293 - val_accuracy: 0.7917 - val_loss: 0.6219\n",
            "Epoch 39/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9169 - loss: 0.2179\n",
            "Epoch 39: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 248ms/step - accuracy: 0.9168 - loss: 0.2179 - val_accuracy: 0.8414 - val_loss: 0.4310\n",
            "Epoch 40/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9148 - loss: 0.2219\n",
            "Epoch 40: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 259ms/step - accuracy: 0.9147 - loss: 0.2218 - val_accuracy: 0.7768 - val_loss: 0.6909\n",
            "Epoch 41/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.9134 - loss: 0.2213\n",
            "Epoch 41: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - accuracy: 0.9134 - loss: 0.2213 - val_accuracy: 0.8698 - val_loss: 0.3712\n",
            "Epoch 42/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9172 - loss: 0.2138\n",
            "Epoch 42: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 259ms/step - accuracy: 0.9171 - loss: 0.2138 - val_accuracy: 0.7744 - val_loss: 0.6822\n",
            "Epoch 43/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9199 - loss: 0.2108\n",
            "Epoch 43: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 258ms/step - accuracy: 0.9197 - loss: 0.2109 - val_accuracy: 0.7827 - val_loss: 0.6982\n",
            "Epoch 44/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.9087 - loss: 0.2272\n",
            "Epoch 44: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 290ms/step - accuracy: 0.9088 - loss: 0.2271 - val_accuracy: 0.6946 - val_loss: 0.9575\n",
            "Epoch 45/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9127 - loss: 0.2290\n",
            "Epoch 45: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 259ms/step - accuracy: 0.9127 - loss: 0.2289 - val_accuracy: 0.7570 - val_loss: 0.7938\n",
            "Epoch 46/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.9114 - loss: 0.2163\n",
            "Epoch 46: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 252ms/step - accuracy: 0.9114 - loss: 0.2163 - val_accuracy: 0.7119 - val_loss: 0.7411\n",
            "Epoch 47/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9138 - loss: 0.2215\n",
            "Epoch 47: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - accuracy: 0.9137 - loss: 0.2215 - val_accuracy: 0.8379 - val_loss: 0.4582\n",
            "Epoch 48/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.9206 - loss: 0.2075\n",
            "Epoch 48: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 296ms/step - accuracy: 0.9205 - loss: 0.2076 - val_accuracy: 0.8025 - val_loss: 0.4916\n",
            "Epoch 49/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.9152 - loss: 0.2109\n",
            "Epoch 49: val_accuracy did not improve from 0.88650\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.9151 - loss: 0.2109 - val_accuracy: 0.8341 - val_loss: 0.3866\n",
            "Epoch 50/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9192 - loss: 0.2068\n",
            "Epoch 50: val_accuracy improved from 0.88650 to 0.89761, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 252ms/step - accuracy: 0.9192 - loss: 0.2068 - val_accuracy: 0.8976 - val_loss: 0.2847\n",
            "Epoch 51/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9183 - loss: 0.2071\n",
            "Epoch 51: val_accuracy did not improve from 0.89761\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 251ms/step - accuracy: 0.9182 - loss: 0.2072 - val_accuracy: 0.8140 - val_loss: 0.5389\n",
            "Epoch 52/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.9163 - loss: 0.2068\n",
            "Epoch 52: val_accuracy did not improve from 0.89761\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 250ms/step - accuracy: 0.9163 - loss: 0.2068 - val_accuracy: 0.8886 - val_loss: 0.3104\n",
            "Epoch 53/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9194 - loss: 0.2057\n",
            "Epoch 53: val_accuracy improved from 0.89761 to 0.90351, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 257ms/step - accuracy: 0.9193 - loss: 0.2058 - val_accuracy: 0.9035 - val_loss: 0.2688\n",
            "Epoch 54/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9212 - loss: 0.2116\n",
            "Epoch 54: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 241ms/step - accuracy: 0.9211 - loss: 0.2116 - val_accuracy: 0.7872 - val_loss: 0.6811\n",
            "Epoch 55/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9143 - loss: 0.2129\n",
            "Epoch 55: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 243ms/step - accuracy: 0.9142 - loss: 0.2128 - val_accuracy: 0.7858 - val_loss: 0.7413\n",
            "Epoch 56/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9171 - loss: 0.2035\n",
            "Epoch 56: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 234ms/step - accuracy: 0.9171 - loss: 0.2035 - val_accuracy: 0.7879 - val_loss: 0.7562\n",
            "Epoch 57/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.9224 - loss: 0.1995\n",
            "Epoch 57: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - accuracy: 0.9224 - loss: 0.1996 - val_accuracy: 0.8539 - val_loss: 0.4156\n",
            "Epoch 58/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9200 - loss: 0.2025\n",
            "Epoch 58: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 256ms/step - accuracy: 0.9199 - loss: 0.2026 - val_accuracy: 0.8816 - val_loss: 0.3212\n",
            "Epoch 59/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9222 - loss: 0.2070\n",
            "Epoch 59: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 272ms/step - accuracy: 0.9221 - loss: 0.2069 - val_accuracy: 0.7928 - val_loss: 0.6142\n",
            "Epoch 60/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9214 - loss: 0.2013\n",
            "Epoch 60: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.9214 - loss: 0.2014 - val_accuracy: 0.8261 - val_loss: 0.3761\n",
            "Epoch 61/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9233 - loss: 0.1988\n",
            "Epoch 61: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 237ms/step - accuracy: 0.9232 - loss: 0.1988 - val_accuracy: 0.8355 - val_loss: 0.3739\n",
            "Epoch 62/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9191 - loss: 0.1999\n",
            "Epoch 62: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 248ms/step - accuracy: 0.9191 - loss: 0.2000 - val_accuracy: 0.8615 - val_loss: 0.4212\n",
            "Epoch 63/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9255 - loss: 0.1918\n",
            "Epoch 63: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9255 - loss: 0.1918 - val_accuracy: 0.9035 - val_loss: 0.3034\n",
            "Epoch 64/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9219 - loss: 0.1931\n",
            "Epoch 64: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 250ms/step - accuracy: 0.9218 - loss: 0.1932 - val_accuracy: 0.8580 - val_loss: 0.3235\n",
            "Epoch 65/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.9248 - loss: 0.1995\n",
            "Epoch 65: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - accuracy: 0.9248 - loss: 0.1995 - val_accuracy: 0.8875 - val_loss: 0.2965\n",
            "Epoch 66/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9225 - loss: 0.1926\n",
            "Epoch 66: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 249ms/step - accuracy: 0.9225 - loss: 0.1927 - val_accuracy: 0.8862 - val_loss: 0.2783\n",
            "Epoch 67/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.9237 - loss: 0.1961\n",
            "Epoch 67: val_accuracy did not improve from 0.90351\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 295ms/step - accuracy: 0.9237 - loss: 0.1961 - val_accuracy: 0.8705 - val_loss: 0.4003\n",
            "Epoch 68/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9237 - loss: 0.1937\n",
            "Epoch 68: val_accuracy improved from 0.90351 to 0.90767, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 249ms/step - accuracy: 0.9237 - loss: 0.1937 - val_accuracy: 0.9077 - val_loss: 0.2501\n",
            "Epoch 69/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9264 - loss: 0.1901\n",
            "Epoch 69: val_accuracy did not improve from 0.90767\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 271ms/step - accuracy: 0.9264 - loss: 0.1900 - val_accuracy: 0.8990 - val_loss: 0.2598\n",
            "Epoch 70/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9178 - loss: 0.2032\n",
            "Epoch 70: val_accuracy did not improve from 0.90767\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.9178 - loss: 0.2031 - val_accuracy: 0.9066 - val_loss: 0.2588\n",
            "Epoch 71/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.9243 - loss: 0.1997\n",
            "Epoch 71: val_accuracy did not improve from 0.90767\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 248ms/step - accuracy: 0.9243 - loss: 0.1995 - val_accuracy: 0.8969 - val_loss: 0.3124\n",
            "Epoch 72/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.9256 - loss: 0.1877\n",
            "Epoch 72: val_accuracy did not improve from 0.90767\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 289ms/step - accuracy: 0.9257 - loss: 0.1877 - val_accuracy: 0.8556 - val_loss: 0.3830\n",
            "Epoch 73/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.9238 - loss: 0.1905\n",
            "Epoch 73: val_accuracy did not improve from 0.90767\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9237 - loss: 0.1906 - val_accuracy: 0.8851 - val_loss: 0.2664\n",
            "Epoch 74/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9237 - loss: 0.1983\n",
            "Epoch 74: val_accuracy improved from 0.90767 to 0.91322, saving model to results/resulting_architecture.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9237 - loss: 0.1981 - val_accuracy: 0.9132 - val_loss: 0.2423\n",
            "Epoch 75/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.9231 - loss: 0.1891\n",
            "Epoch 75: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.9231 - loss: 0.1891 - val_accuracy: 0.9108 - val_loss: 0.2438\n",
            "Epoch 76/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.9247 - loss: 0.1890\n",
            "Epoch 76: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 243ms/step - accuracy: 0.9246 - loss: 0.1890 - val_accuracy: 0.8723 - val_loss: 0.3484\n",
            "Epoch 77/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9240 - loss: 0.1847\n",
            "Epoch 77: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 258ms/step - accuracy: 0.9240 - loss: 0.1847 - val_accuracy: 0.8709 - val_loss: 0.3286\n",
            "Epoch 78/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9238 - loss: 0.1892\n",
            "Epoch 78: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 244ms/step - accuracy: 0.9238 - loss: 0.1891 - val_accuracy: 0.8938 - val_loss: 0.2570\n",
            "Epoch 79/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9232 - loss: 0.1869\n",
            "Epoch 79: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - accuracy: 0.9232 - loss: 0.1870 - val_accuracy: 0.9028 - val_loss: 0.2454\n",
            "Epoch 80/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9243 - loss: 0.1856\n",
            "Epoch 80: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 253ms/step - accuracy: 0.9243 - loss: 0.1856 - val_accuracy: 0.8921 - val_loss: 0.3407\n",
            "Epoch 81/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9193 - loss: 0.1883\n",
            "Epoch 81: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9194 - loss: 0.1883 - val_accuracy: 0.8421 - val_loss: 0.4390\n",
            "Epoch 82/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9259 - loss: 0.1908\n",
            "Epoch 82: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.9259 - loss: 0.1908 - val_accuracy: 0.8823 - val_loss: 0.3488\n",
            "Epoch 83/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9238 - loss: 0.1843\n",
            "Epoch 83: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9238 - loss: 0.1843 - val_accuracy: 0.8136 - val_loss: 0.7747\n",
            "Epoch 84/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9250 - loss: 0.1854\n",
            "Epoch 84: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.9251 - loss: 0.1853 - val_accuracy: 0.8841 - val_loss: 0.3206\n",
            "Epoch 85/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9286 - loss: 0.1802\n",
            "Epoch 85: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 300ms/step - accuracy: 0.9287 - loss: 0.1801 - val_accuracy: 0.8723 - val_loss: 0.3480\n",
            "Epoch 86/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.9312 - loss: 0.1706\n",
            "Epoch 86: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.9312 - loss: 0.1707 - val_accuracy: 0.8980 - val_loss: 0.3314\n",
            "Epoch 87/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9230 - loss: 0.1869\n",
            "Epoch 87: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 256ms/step - accuracy: 0.9230 - loss: 0.1868 - val_accuracy: 0.8865 - val_loss: 0.3102\n",
            "Epoch 88/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.9275 - loss: 0.1810\n",
            "Epoch 88: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 293ms/step - accuracy: 0.9275 - loss: 0.1809 - val_accuracy: 0.8754 - val_loss: 0.3096\n",
            "Epoch 89/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.9271 - loss: 0.1785\n",
            "Epoch 89: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 251ms/step - accuracy: 0.9271 - loss: 0.1785 - val_accuracy: 0.8497 - val_loss: 0.4332\n",
            "Epoch 90/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.9274 - loss: 0.1810\n",
            "Epoch 90: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 251ms/step - accuracy: 0.9274 - loss: 0.1811 - val_accuracy: 0.9025 - val_loss: 0.3316\n",
            "Epoch 91/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9333 - loss: 0.1705\n",
            "Epoch 91: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 250ms/step - accuracy: 0.9333 - loss: 0.1706 - val_accuracy: 0.9000 - val_loss: 0.2937\n",
            "Epoch 92/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9289 - loss: 0.1843\n",
            "Epoch 92: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 258ms/step - accuracy: 0.9289 - loss: 0.1843 - val_accuracy: 0.8931 - val_loss: 0.3005\n",
            "Epoch 93/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9280 - loss: 0.1790\n",
            "Epoch 93: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 248ms/step - accuracy: 0.9280 - loss: 0.1790 - val_accuracy: 0.9070 - val_loss: 0.2980\n",
            "Epoch 94/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9296 - loss: 0.1758\n",
            "Epoch 94: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 255ms/step - accuracy: 0.9296 - loss: 0.1759 - val_accuracy: 0.8938 - val_loss: 0.3708\n",
            "Epoch 95/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9301 - loss: 0.1767\n",
            "Epoch 95: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 259ms/step - accuracy: 0.9301 - loss: 0.1768 - val_accuracy: 0.8275 - val_loss: 0.4357\n",
            "Epoch 96/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9345 - loss: 0.1724\n",
            "Epoch 96: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 252ms/step - accuracy: 0.9345 - loss: 0.1725 - val_accuracy: 0.8924 - val_loss: 0.3302\n",
            "Epoch 97/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.9284 - loss: 0.1816\n",
            "Epoch 97: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 253ms/step - accuracy: 0.9283 - loss: 0.1816 - val_accuracy: 0.8646 - val_loss: 0.3691\n",
            "Epoch 98/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.9300 - loss: 0.1788\n",
            "Epoch 98: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - accuracy: 0.9301 - loss: 0.1787 - val_accuracy: 0.8976 - val_loss: 0.3001\n",
            "Epoch 99/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9317 - loss: 0.1718\n",
            "Epoch 99: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 249ms/step - accuracy: 0.9317 - loss: 0.1719 - val_accuracy: 0.9035 - val_loss: 0.2898\n",
            "Epoch 100/100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9349 - loss: 0.1718\n",
            "Epoch 100: val_accuracy did not improve from 0.91322\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 247ms/step - accuracy: 0.9349 - loss: 0.1718 - val_accuracy: 0.8282 - val_loss: 0.5613\n",
            "\n",
            "max val acc: 0.913\n",
            "\n",
            "Keras model saved in: results/resulting_architecture.h5\n",
            "\n",
            "Elapsed time (training): 0:26:02.564468\n",
            "\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 6724 files for training.\n",
            "Found 9605 files belonging to 2 classes.\n",
            "Using 2881 files for validation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpmt4ybnas'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134688277142928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277144272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277143312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277145040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277143120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277142160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277145808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277144656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277145424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277146192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277145616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277146768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277146384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277146960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277148688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277148880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277146000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277144848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277147344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277147152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277147536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277150032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277149840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277148496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277149648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134688277149072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tflite model saved in: /content/results/resulting_architecture.tflite\n",
            "\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9225 - loss: 0.2236\n",
            "\n",
            "Keras model test accuracy: 0.9110000133514404\n",
            "\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tflite model test accuracy: 0.91\n",
            "\n",
            "Tflite model in: /content/results/resulting_architecture.tflite\n",
            "\n"
          ]
        }
      ]
    }
  ]
}